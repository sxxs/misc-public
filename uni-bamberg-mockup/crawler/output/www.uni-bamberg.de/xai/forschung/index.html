<!DOCTYPE html>
<html lang="de">
<head>

<meta charset="utf-8">
<!-- 
	featured by elementare teilchen GmbH, www.elementare-teilchen.de

	This website is powered by TYPO3 - inspiring people to share!
	TYPO3 is a free open source Content Management Framework initially created by Kasper Skaarhoj and licensed under GNU/GPL.
	TYPO3 is copyright 1998-2025 of Kasper Skaarhoj. Extensions are copyright of their respective owners.
	Information and contribution at https://typo3.org/
-->



<title>Forschung - Lehrstuhl für Erklärbares Maschinelles Lernen </title>
<meta name="generator" content="TYPO3 CMS">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="revisit-after" content="7 days">


<link rel="stylesheet" href="/typo3temp/assets/css/0e161fa31ef1359d8d20987651f196ee.css?1728045045" media="all" nonce="-JoiRsgFeENKiL1nfI_B3lZoEHx3qBjm-J3EQfgEHVJwHOxaXQzgwQ">
<link rel="stylesheet" href="/_assets/8ff03e32fdbff3962349cc6e17c99f7d/Css/MediaAlbum.css?1732185887" media="all" nonce="-JoiRsgFeENKiL1nfI_B3lZoEHx3qBjm-J3EQfgEHVJwHOxaXQzgwQ">
<link rel="stylesheet" href="/_assets/47400bb84e4e0324cd1d0a84db430fe5/Css/styles.css?1741641568" media="all" nonce="-JoiRsgFeENKiL1nfI_B3lZoEHx3qBjm-J3EQfgEHVJwHOxaXQzgwQ">
<link href="/_frontend/bundled/assets/@fancyapps-DTpptOfq.css?1765795199" rel="stylesheet" nonce="-JoiRsgFeENKiL1nfI_B3lZoEHx3qBjm-J3EQfgEHVJwHOxaXQzgwQ" >
<link href="/_frontend/bundled/assets/fancyboxCss-Bu88iTds.css?1765795199" rel="stylesheet" nonce="-JoiRsgFeENKiL1nfI_B3lZoEHx3qBjm-J3EQfgEHVJwHOxaXQzgwQ" >
<link href="/_frontend/bundled/assets/screen-F1MkGt4N.css?1765795199" rel="stylesheet" nonce="-JoiRsgFeENKiL1nfI_B3lZoEHx3qBjm-J3EQfgEHVJwHOxaXQzgwQ" >






<link rel="icon" type="image/png" href="/_frontend/siteunibamberg/favicon/favicon-96x96.png" sizes="96x96" />
        <link rel="icon" type="image/svg+xml" href="/_frontend/siteunibamberg/favicon/favicon.svg" />
        <link rel="shortcut icon" href="/_frontend/siteunibamberg/favicon/favicon.ico" />
        <link rel="apple-touch-icon" sizes="180x180" href="/_frontend/siteunibamberg/favicon/apple-touch-icon.png" />
        <link rel="manifest" href="">
<link rel="canonical" href="https://www.uni-bamberg.de/xai/forschung/"/>

<link rel="alternate" hreflang="de" href="https://www.uni-bamberg.de/xai/forschung/"/>
<link rel="alternate" hreflang="en-US" href="https://www.uni-bamberg.de/en/ai/chair-of-explainable-machine-learning/research/"/>
<link rel="alternate" hreflang="x-default" href="https://www.uni-bamberg.de/xai/forschung/"/>

<script type="application/ld+json">[{"@context":"https:\/\/www.schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"item":{"@id":"https:\/\/www.uni-bamberg.de\/","name":"Startseite"}},{"@type":"ListItem","position":2,"item":{"@id":"https:\/\/www.uni-bamberg.de\/fakultaeten\/","name":"Fakult\u00e4ten"}},{"@type":"ListItem","position":3,"item":{"@id":"https:\/\/www.uni-bamberg.de\/wiai\/","name":"Wirtschaftsinformatik und Angewandte Informatik"}},{"@type":"ListItem","position":4,"item":{"@id":"https:\/\/www.uni-bamberg.de\/wiai\/faecher\/","name":"Professuren und Lehrst\u00fchle"}},{"@type":"ListItem","position":5,"item":{"@id":"https:\/\/www.uni-bamberg.de\/ai\/","name":"Angewandte Informatik"}},{"@type":"ListItem","position":6,"item":{"@id":"https:\/\/www.uni-bamberg.de\/xai\/","name":"Lehrstuhl f\u00fcr Erkl\u00e4rbares Maschinelles Lernen"}},{"@type":"ListItem","position":7,"item":{"@id":"https:\/\/www.uni-bamberg.de\/xai\/forschung\/","name":"Forschung"}}]}]</script>
</head>
<body class="page  page--default  act-responsive "
	      data-page="150582" data-rootline="18944 62 63 132279 2923 150565 150582 "><nav class="page__skiplinks"><ul><li><a href="#nav-main">zur Hauptnavigation springen</a></li><li><a href="#content-main">zum Inhaltsbereich springen</a></li></ul></nav><div class="responsive"></div><div id="top" class="page__wrapper"><header class="page-head-mobile no-print"><div data-headroom class="page-head-mobile__item  clearfix "><nav class="page-head-mobile__nav"><a class="page-head-mobile__nav__link  js-show-nav" href="#" title="Navigation"></a></nav><div class="page-head-mobile__brand"><a href="/" title="Universität Bamberg">Universität Bamberg</a></div><a class="[ page-head__search-hint ] js-search-toggle" href="#" title="Suche"><span class="u-visually-hidden">
						Suche öffnen
					</span></a></div><div class="page-content__nav-parents"><div class="nav-parents__button"><a href="#" class="nav-parents__button--link  [ link-list__link ] js-show-nav-parents" title="Navigation">
						Sie befinden sich hier:
					</a></div><nav class="nav-parents"><ul class="nav-parents__list  [ link-list ]"><li class="[ nav-parents__item  nav-parent__item--parent ]  link-list__item" ><a href="/" class="nav-parents__link  [ link-list__link ]" title="Modern studieren, international forschen, innovativ lehren und gelassen leben in Bamberg. Das Welterbe ist unser Campus &amp;#127891;">Startseite</a></li><li class="[ nav-parents__item  nav-parent__item--parent ]  link-list__item" ><a href="/fakultaeten/" class="nav-parents__link  [ link-list__link ]" title="Die vier Fakultäten der Uni: Geistes- und Kulturwissenschaften, Sozial- und Wirtschaftswissenschaften, Humanwissenschaften, Wirtschaftsinformatik">Fakultäten</a></li><li class="[ nav-parents__item  nav-parent__item--parent ]  link-list__item" ><a href="/wiai/" class="nav-parents__link  [ link-list__link ]" title="Die Fakultät WIAI verbindet die Wirtschaftsinformatik mit Angewandter Informatik und der klassischen Theoretischen und Praktischen Informatik">Wirtschaftsinformatik und Angewandte Informatik</a></li><li class="[ nav-parents__item  nav-parent__item--parent ]  link-list__item" ><a href="/wiai/faecher/" class="nav-parents__link  [ link-list__link ]" title="Überblick über die Fächer an der Fakultät Wirtschaftsinformatik und Angewandte Informatik">Professuren und Lehrstühle</a></li><li class="[ nav-parents__item  nav-parent__item--parent ]  link-list__item" ><a href="/ai/" class="nav-parents__link  [ link-list__link ]" title="Forschung zu KI, Medien- und Kulturinformatik, Mensch-Computer-Interaktion sowie Grafik, Design und Visualisierung">Angewandte Informatik</a></li><li class="[ nav-parents__item  nav-parent__item--parent ]  link-list__item" ><a href="/xai/" class="nav-parents__link  [ link-list__link ]" title="Lehrstuhl für Erklärbares Maschinelles Lernen">Lehrstuhl für Erklärbares Maschinelles Lernen</a></li><li class="[ nav-parents__item  nav-parent__item--parent ]  link-list__item" ><a href="/xai/forschung/" class="nav-parents__link  [ link-list__link ]" title="Forschung">Forschung</a></li></ul></nav></div><div class="[ page-head__search  page-head__search--mobile ]  js-search"><form class="search-global search-global--mobile js-search-global-form" action="https://www.google.com"><fieldset><label class="search-global__option js-search-option is-active" data-search-option="Unibamberg">
                    www.uni-bamberg.de
                    <input type="radio" name="searchOption" value="www.uni-bamberg.de"/></label><label class="search-global__option js-search-option" data-search-option="Univis">
                    univis.uni-bamberg.de
                    <input type="radio" name="searchOption" value="univis.uni-bamberg.de"/></label><label class="search-global__option js-search-option" data-search-option="Fis">
                    fis.uni-bamberg.de
                    <input type="radio" name="searchOption" value="fis.uni-bamberg.de"/></label></fieldset><fieldset><input name="q" type="text" class="search__input js-search-global-input" placeholder="Mit Google suchen" aria-label="search"/><button type="submit" class="search__submit  js-search-global-submit" title="Suche starten"><span class="u-visually-hidden">Suche starten</span></button><div class="search__engines-container"><label class="search__engine  js-search-engine is-active" data-search-engine="Google" aria-label="Google"><input class="u-visually-hidden" type="radio" name="searchEngine" value="https://www.google.de/search"><span class="search__engine-icon search__engine-icon-google"></span></label><label class="search__engine  js-search-engine" data-search-engine="Ecosia" aria-label="Ecosia"><input class="u-visually-hidden" type="radio" name="searchEngine" value="https://www.ecosia.org/search"><span class="search__engine-icon search__engine-icon-ecosia"></span></label><label class="search__engine  js-search-engine" data-search-engine="Bing" aria-label="Bing"><input class="u-visually-hidden" type="radio" name="searchEngine" value="https://www.bing.com/search"><span class="search__engine-icon search__engine-icon-bing"></span></label></div></fieldset><button type="button" class="search-global__close js-search-toggle"><span class="u-visually-hidden">Suche schließen</span></button></form></div></header><header class="page-head"><div class="page-head__item  clearfix  [ box  box--large  box--brand-1st ]  [ rounded--topright--large  rounded--topleft ]  gradient--header"><div class="page-head__brand"><div class="page-head__logo"><a href="/" title="Universität Bamberg"><span class="page-head__logo__img page-head__logo__img--de">&nbsp; </span> Universität Bamberg</a></div></div><div class="page-head__useractions no-print"><div class="[ page-head__contrast ]"><div class="contrastx"><et-state-toggle active-class="t-contrast" inactive-class="" cookie="contrastTheme" el="body"><button
                                    type="button"
                                    class="contrast__button"
                                >
                                    Kontrast
                                </button></et-state-toggle></div></div><div class="[ page-head__language ]"><nav class="nav-language"><ul class="[ list-inline  link-list  list-inline--delimited  list-inline--delimited-shadow  list-inline--delimited-bright  list-inline--delimited-large  ][ nav-language__list ]  js-lang-list"><li data-testid="language-link" class="list-inline__item  nav-language__item--mobile  [ c-nav-lang__link  c-nav-lang__link--active ]">Deutsch</li><li data-testid="language-link" class="list-inline__item  nav-language__item--mobile [ c-nav-lang__link ]"><a href="/en/ai/chair-of-explainable-machine-learning/research/">English</a></li></ul></nav></div><div class="[ page-head__search ]"><button class="search__toggle js-search-toggle" type="button"><span class="u-visually-hidden">Suche öffnen</span></button></div></div><div class="[ page-head__search-overlay ] rounded--topleft rounded--topright--large js-search"><form class="search-global js-search-global-form" action="https://www.google.com"><fieldset><label class="search-global__option js-search-option is-active" data-search-option="Unibamberg">
                    www.uni-bamberg.de
                    <input type="radio" name="searchOption" value="www.uni-bamberg.de"/></label><label class="search-global__option js-search-option" data-search-option="Univis">
                    univis.uni-bamberg.de
                    <input type="radio" name="searchOption" value="univis.uni-bamberg.de"/></label><label class="search-global__option js-search-option" data-search-option="Fis">
                    fis.uni-bamberg.de
                    <input type="radio" name="searchOption" value="fis.uni-bamberg.de"/></label></fieldset><fieldset><input name="q" type="text" class="search__input js-search-global-input" placeholder="Mit Google suchen" aria-label="search"/><button type="submit" class="search__submit  js-search-global-submit" title="Suche starten"><span class="u-visually-hidden">Suche starten</span></button><div class="search__engines-container"><label class="search__engine  js-search-engine is-active" data-search-engine="Google" aria-label="Google"><input class="u-visually-hidden" type="radio" name="searchEngine" value="https://www.google.de/search"><span class="search__engine-icon search__engine-icon-google"></span></label><label class="search__engine  js-search-engine" data-search-engine="Ecosia" aria-label="Ecosia"><input class="u-visually-hidden" type="radio" name="searchEngine" value="https://www.ecosia.org/search"><span class="search__engine-icon search__engine-icon-ecosia"></span></label><label class="search__engine  js-search-engine" data-search-engine="Bing" aria-label="Bing"><input class="u-visually-hidden" type="radio" name="searchEngine" value="https://www.bing.com/search"><span class="search__engine-icon search__engine-icon-bing"></span></label></div></fieldset><button type="button" class="search-global__close js-search-toggle"><span class="u-visually-hidden">Suche schließen</span></button></form></div></div><div class="page-head__sector  [ box  box--brand-1st-light ] "><div class="sector__logo"></div><p class="[ sector__title  sector__title2 ]"><a href="/wiai/">Fakultät Wirtschaftsinformatik und Angewandte Informatik</a></p><p class="[ sector__title  sector__title1 ]"><a href="/xai/">Lehrstuhl für Erklärbares Maschinelles Lernen</a></p></div><div class="page-head__breadcrumb  [ stripe ]  js-breadcrumb"><ol class="nav-breadcrumb  nav-breadcrumb--delimited list-inline"><li class="nav-breadcrumb__item"><a href="/" class="icon--home">&nbsp;<span class="u-visually-hidden">Otto-Friedrich-Universität Bamberg</span></a></li><li class="nav-breadcrumb__item" ><a href="/fakultaeten/" class="nav-breadcrumb__link" title="Die vier Fakultäten der Uni: Geistes- und Kulturwissenschaften, Sozial- und Wirtschaftswissenschaften, Humanwissenschaften, Wirtschaftsinformatik">Fakultäten</a></li><li class="nav-breadcrumb__item" ><a href="/wiai/" class="nav-breadcrumb__link" title="Die Fakultät WIAI verbindet die Wirtschaftsinformatik mit Angewandter Informatik und der klassischen Theoretischen und Praktischen Informatik">Wirtschaftsinformatik und Angewandte Informatik</a></li><li class="nav-breadcrumb__item" ><a href="/wiai/faecher/" class="nav-breadcrumb__link" title="Überblick über die Fächer an der Fakultät Wirtschaftsinformatik und Angewandte Informatik">Professuren und Lehrstühle</a></li><li class="nav-breadcrumb__item" ><a href="/ai/" class="nav-breadcrumb__link" title="Forschung zu KI, Medien- und Kulturinformatik, Mensch-Computer-Interaktion sowie Grafik, Design und Visualisierung">Angewandte Informatik</a></li><li class="nav-breadcrumb__item" ><a href="/xai/" class="nav-breadcrumb__link" title="Lehrstuhl für Erklärbares Maschinelles Lernen">Lehrstuhl für Erklärbares Maschinelles Lernen</a></li><li class="[ nav-breadcrumb__item  nav-breadcrumb__item--active ]">Forschung</li></ol></div></header><main class="page-content"><div class="page-content__nav" id="nav-main"><!-- Start nav-mobile --><nav class="nav-mobile  js-nav-panel" data-ui-component="Mobile navigation"><div class="page-content__sector [ box  box--brand-1st-light ] ">
						Seitenbereich: 
						<p class="[ sector__title  sector__title2 ]"><a href="/wiai/">Fakultät Wirtschaftsinformatik und Angewandte Informatik</a></p><p class="[ sector__title  sector__title1 ]"><a href="/xai/">Lehrstuhl für Erklärbares Maschinelles Lernen</a></p></div><div class="page-content__useractions"><et-state-toggle active-class="t-contrast" inactive-class="" cookie="contrastTheme" el="body"><button type="button" class="page-content__contrast contrast__button  link-list__item__link contrastx--mobile  contrastx">
                                Kontrast
                            </button></et-state-toggle><button class="[ page-content__language-hint ]  js-toggle-lang">
							Sprache
						</button><div class="page-content__language"><nav class=" [ nav-language  nav-language--mobile ] js-lang"><ul class="[ list-inline  link-list  list-inline--delimited  list-inline--delimited-shadow  list-inline--delimited-bright  list-inline--delimited-large  ][ nav-language__list ]  js-lang-list"><li data-testid="language-link" class="list-inline__item  nav-language__item--mobile  [ c-nav-lang__link  c-nav-lang__link--active ]">Deutsch</li><li data-testid="language-link" class="list-inline__item  nav-language__item--mobile [ c-nav-lang__link ]"><a href="/en/ai/chair-of-explainable-machine-learning/research/">English</a></li></ul></nav></div></div><nav class="nav-sector " data-ui-component="Sector navigation"><ul class="nav-sector__container  [ link-list ]"><li class="[ nav-sector__item  nav-sector__item--parent  nav-sector__item--flyout  nav-sector__item--first ]  [ link-list__item ]" data-page="150575"><a href="/xai/mitarbeiter/" class="nav-sector__link  [ link-list__link ]  js-nav-sector__link--has-subpages"><span class="nav-sector__link-text">Mitarbeiter</span><span class="nav-sector__expand js-expand"></span></a><ul class="[ nav-sector__container  nav-sector__container--level-2 ]  [ link-list ]"><li class="[ nav-sector__item  nav-sector__item--flyout  nav-sector__item--first ]  link-list__item" data-page="150577"><a href="/xai/mitarbeiter/prof-dr-christian-ledig/" class="nav-sector__link  [ link-list__link ]"><span class="nav-sector__link-text">Prof. Dr. Christian Ledig</span></a></li><li class="[ nav-sector__item  nav-sector__item--flyout  ]  link-list__item" data-page="152583"><a href="/xai/mitarbeiter/franziska-duesel-sekretariat/" class="nav-sector__link  [ link-list__link ]"><span class="nav-sector__link-text">Franziska Düsel (Sekretariat)</span></a></li><li class="[ nav-sector__item  nav-sector__item--flyout  ]  link-list__item" data-page="160340"><a href="/xai/mitarbeiter/sebastian-doerrich-msc/" class="nav-sector__link  [ link-list__link ]"><span class="nav-sector__link-text">Sebastian Dörrich, M.Sc.</span></a></li><li class="[ nav-sector__item  nav-sector__item--flyout  ]  link-list__item" data-page="160805"><a href="/xai/mitarbeiter/francesco-di-salvo-msc/" class="nav-sector__link  [ link-list__link ]"><span class="nav-sector__link-text">Francesco Di Salvo, M.Sc.</span></a></li><li class="[ nav-sector__item  nav-sector__item--flyout  ]  link-list__item" data-page="165766"><a href="/xai/mitarbeiter/jonas-alle-msc/" class="nav-sector__link  [ link-list__link ]"><span class="nav-sector__link-text">Jonas Alle, M.Sc.</span></a></li><li class="[ nav-sector__item  nav-sector__item--flyout  ]  link-list__item" data-page="174054"><a href="/xai/mitarbeiter/my-nguyen-msc/" class="nav-sector__link  [ link-list__link ]"><span class="nav-sector__link-text">My Nguyen, M.Sc.</span></a></li><li class="[ nav-sector__item  nav-sector__item--flyout  nav-sector__item--last ]  link-list__item" data-page="174261"><a href="/xai/mitarbeiter/marco-lents-msc/" class="nav-sector__link  [ link-list__link ]"><span class="nav-sector__link-text">Marco Lents, M.Sc.</span></a></li></ul></li><li class="[ nav-sector__item  nav-sector__item--parent  nav-sector__item--flyout ]  [ link-list__item ]" data-page="150583"><a href="/xai/studium/" class="nav-sector__link  [ link-list__link ]  js-nav-sector__link--has-subpages"><span class="nav-sector__link-text">Studium</span><span class="nav-sector__expand js-expand"></span></a><ul class="[ nav-sector__container  nav-sector__container--level-2 ]  [ link-list ]"><li class="[ nav-sector__item  nav-sector__item--flyout  nav-sector__item--first ]  link-list__item" data-page="150585"><a href="/xai/studium/lehrveranstaltungen/" class="nav-sector__link  [ link-list__link ]"><span class="nav-sector__link-text">Lehrveranstaltungen</span></a></li><li class="[ nav-sector__item  nav-sector__item--flyout  nav-sector__item--last ]  link-list__item" data-page="163200"><a href="/xai/studium/abschlussarbeiten/" class="nav-sector__link  [ link-list__link ]"><span class="nav-sector__link-text">Abschlussarbeiten</span></a></li></ul></li><li class="[ nav-sector__item  nav-sector__item--flyout  ]  link-list__item" data-page="150582"><a href="/xai/forschung/" class="nav-sector__link  [ link-list__link ]"><span class="nav-sector__link-text">Forschung</span></a></li><li class="[ nav-sector__item  nav-sector__item--parent  nav-sector__item--flyout ]  [ link-list__item ]" data-page="152586"><a href="/xai/software-datensaetze/" class="nav-sector__link  [ link-list__link ]  js-nav-sector__link--has-subpages"><span class="nav-sector__link-text">Software & Datensätze</span><span class="nav-sector__expand js-expand"></span></a><ul class="[ nav-sector__container  nav-sector__container--level-2 ]  [ link-list ]"><li class="[ nav-sector__item  nav-sector__item--flyout  nav-sector__item--first ]  link-list__item" data-page="152588"><a href="/xai/software-datensaetze/malpem/" class="nav-sector__link  [ link-list__link ]"><span class="nav-sector__link-text">Software: Multi-Atlas Label Propagation with EM-refinement (MALPEM)</span></a></li><li class="[ nav-sector__item  nav-sector__item--flyout  ]  link-list__item" data-page="152589"><a href="/xai/software-datensaetze/patch-based-evaluation-of-image-segmentation-peis/" class="nav-sector__link  [ link-list__link ]"><span class="nav-sector__link-text">Software: Patch-based Evaluation of Image Segmentation (PEIS)</span></a></li><li class="[ nav-sector__item  nav-sector__item--flyout  nav-sector__item--last ]  link-list__item" data-page="152587"><a href="/xai/software-datensaetze/malpem-adni-features-binary-masks-segmentations-for-5074-adni-subjects/" class="nav-sector__link  [ link-list__link ]"><span class="nav-sector__link-text">Dataset: MALPEM-ADNI: Features, binary masks, segmentations for 5074 ADNI subjects</span></a></li></ul></li><li class="[ nav-sector__item  nav-sector__item--flyout  ]  link-list__item" data-page="152585"><a href="/xai/publikationen/" class="nav-sector__link  [ link-list__link ]"><span class="nav-sector__link-text">Publikationen</span></a></li><li class="[ nav-sector__item  nav-sector__item--flyout  ]  link-list__item" data-page="152615"><a href="/xai/offene-stellen/" class="nav-sector__link  [ link-list__link ]"><span class="nav-sector__link-text">Offene Stellen</span></a></li><li class="[ nav-sector__item  nav-sector__item--flyout  ]  link-list__item" data-page="150571"><a href="/xai/news/" class="nav-sector__link  [ link-list__link ]"><span class="nav-sector__link-text">News</span></a></li><li class="[ nav-sector__item  nav-sector__item--flyout  nav-sector__item--last ]  link-list__item" data-page="172245"><a href="/xai/in-der-presse/" class="nav-sector__link  [ link-list__link ]"><span class="nav-sector__link-text">In der Presse</span></a></li></ul></nav></nav><!-- End nav-mobile --><aside class="page-content__nav-links  base-padding--mobile"></aside></div><!--TYPO3SEARCH_begin--><article id="content-main" class="page-content__content" ><section class="page-content__main page-content__main--full"><div id="c555025" class="ce  
"></div><div id="c555024" class="ce  
"><h2 class="">
	        
                Forschungsschwerpunkte
	        
	        </h2><div class="ce-textpic ce-center ce-above"><div class="ce-bodytext"><p>Wir beschäftigen uns insbesondere mit:</p><ul><li>Entwicklung von robusten, generalisierbaren Neuronalen Netzen (CNNs, Deep Learning)</li><li>Daten-/Annotation-effiziente Modelle basierend auf Semi-/Self-supervised Learning</li><li>Outlier-Detektion und Imputation von unvollständige Datensätzen</li><li>Rekonstruktion von Bild- und&nbsp;Videodaten, eg. mit Hilfe von Super-Resolution</li><li>Segmentierungsprobleme, insbesondere MRI Brain Segmentation</li><li>Quantifizierung von Unsicherheiten von Klassifizierungsvorhersagen</li><li>Entwicklung von interpretierbaren Features zur Verbesserung der Anwender-/Patientenkommunikation</li><li>Evaluierung von Algorithmus&nbsp;Performance und Quantifizierung von&nbsp;Data-biases</li><li>Translation von Forschungsergebnissen in industrielle oder medizinische Kontexte</li><li>Quantifizierung&nbsp;von menschlicher&nbsp;Anatomie anhand&nbsp;von Bilddaten (MRI, X-Ray, CT)&nbsp;im Kontext von&nbsp;Erkrankungen wie Demenz, Tumoren und Traumata.</li></ul></div></div></div><div id="c682814" class="csc-content"><h2 class="">
	        
                Ausgewählte Forschungsarbeiten
	        
	        </h2><div class="panel-group  js-accordion js-accordion-toggle-all-referrer" id="accordion-682814"><button
							class="js-accordion__toggle-all"
							type="button"
							data-accordion-expand-all="Alle aufklappen"
							data-accordion-collapse-all="Alle zuklappen"
						>Alle aufklappen
						</button><div class="panel  panel-default"><div class="panel-heading" id="panel-heading-682814-0"><h4 class="panel-title"><button class="collapsed"
                                            type="button"
                                            data-toggle="collapse"
                                            data-parent="#accordion-682814"
                                            data-target="#panel-682814-0"
                                            aria-controls="panel-682814-0"
                                            aria-expanded="false"
                                    >
                                        F. Di Salvo, M. Nguyen, C. Ledig, "Embedding-Based Federated Data Sharing via Differentially Private Conditional VAEs", MICCAI 2025
                                    </button></h4></div><div id="panel-682814-0"
                                 class="panel-collapse  collapse"
                                 aria-labelledby="panel-heading-682814-0"><div class="panel-body"><div id="c690178" class="  
"><div class="ce-textpic ce-right ce-intext ce-nowrap"><div class="[ ce-gallery  ]" data-ce-columns="1" data-ce-images="1"><div class="ce-gallery__content"><div class="ce-column"><figure data-ce-type="pdf"><div class="ce-media-copyright-wrapper"><et-copyright copyright-position="bottom-right"><a href="/fileadmin/_processed_/9/3/csm_2025_disalvo_miccai_PullFigure_0ab9e06cb1.webp" data-fancybox="gallery690178" data-caption="" aria-label="F. Di Salvo, M. Nguyen, C. Ledig, &quot;Embedding-Based Federated Data Sharing via Differentially Private Conditional VAEs&quot;, MICCAI 2025 Gallerie öffnen" data-media="(max-width: 520px);(max-width: 767px);(max-width: 1199px);(max-width: 1399px);(min-width: 1400px)" data-sources="/fileadmin/_processed_/9/3/csm_2025_disalvo_miccai_PullFigure_062783aa08.webp;
                            /fileadmin/_processed_/9/3/csm_2025_disalvo_miccai_PullFigure_96129ed2d5.webp;
                            /fileadmin/_processed_/9/3/csm_2025_disalvo_miccai_PullFigure_295bb166ca.webp;
                            /fileadmin/_processed_/9/3/csm_2025_disalvo_miccai_PullFigure_5a316e7656.webp;
                        
                        /fileadmin/_processed_/9/3/csm_2025_disalvo_miccai_PullFigure_4217c3e5b2.webp" class="fancybox-popup"><picture><source srcset="/fileadmin/_processed_/9/3/csm_2025_disalvo_miccai_PullFigure_ca14d3fdb5.webp" media="(min-width: 981px)" type="image/webp"><source srcset="
                                
                                /fileadmin/_processed_/9/3/csm_2025_disalvo_miccai_PullFigure_4a41b378cc.webp 474w,
                                /fileadmin/_processed_/9/3/csm_2025_disalvo_miccai_PullFigure_59827a94f4.webp 274w,
                                /fileadmin/_processed_/9/3/csm_2025_disalvo_miccai_PullFigure_fadfb84a3d.webp 200w"
                        sizes="calc(100vw - 30px)"
                        type="image/webp"><img data-cr="xAILab Bamberg" class="image-embed-item " loading="lazy" src="/fileadmin/_processed_/9/3/csm_2025_disalvo_miccai_PullFigure_c53c62e027.webp" width="570" height="239" alt="" /></picture></a><span slot="copyright-text" class="ce-copyright">xAILab Bamberg</span></et-copyright></div></figure></div></div></div><div class="ce-bodytext"><h2 class="">
	        
                F. Di Salvo, M. Nguyen, C. Ledig, "Embedding-Based Federated Data Sharing via Differentially Private Conditional VAEs", MICCAI 2025
	        
	        </h2><p>Deep Learning (DL) hat die medizinische Bildgebung revolutioniert. Doch seine Einführung wird durch Datenknappheit und Datenschutzbestimmungen eingeschränkt, wodurch der Zugang zu vielfältigen Datensätzen begrenzt ist. Federated Learning (FL) ermöglicht zwar ein dezentrales Training, leidet jedoch unter hohen Kommunikationskosten und ist oft auf eine einzige nachgelagerte Aufgabe beschränkt, was die Flexibilität einschränkt. Wir schlagen eine Methode zum Datenaustausch über differentiell private (DP) generative Modelle vor. Mithilfe von Foundation-Modellen extrahieren wir kompakte, informative Einbettungen, wodurch Redundanzen reduziert und der Rechenaufwand gesenkt werden. Kunden trainieren gemeinsam einen Differentially Private Conditional Variational Autoencoder (DP-CVAE), um eine globale, datenschutzbewusste Datenverteilung zu modellieren, die verschiedene nachgelagerte Aufgaben unterstützt. Unser Ansatz, der für mehrere Feature-Extraktoren validiert wurde, verbessert den Datenschutz, die Skalierbarkeit und die Effizienz. Er übertrifft dabei herkömmliche FL-Klassifikatoren, während gleichzeitig die differentielle Privatsphäre gewährleistet ist. Darüber hinaus erzeugt der DP-CVAE-Embedder mit höherer Genauigkeit als der DP-CGAN-Embedder und benötigt dabei fünfmal weniger Parameter.</p><p>Autoren: <a href="https://francescodisalvo05.github.io" target="_blank" rel="noreferrer">Francesco Di Salvo</a>*, <a href="https://scholar.google.com/citations?user=eGWmUVEAAAAJ&amp;hl=en" target="_blank" rel="noreferrer">My Nguyen</a>*, <a href="http://www.christianledig.com/" target="_blank" rel="noreferrer">Christian Ledig</a></p><p>* gemeinsame Urheberschaft</p><p><a href="https://arxiv.org/pdf/2507.02671" target="_blank" rel="noreferrer">[Preprint]</a>, <a href="/fileadmin/xai/research/disalvo_miccai2025.bib">[Bibtex]</a><span class="filesize">(261.0 B)</span></p></div></div></div></div></div></div><div class="panel  panel-default"><div class="panel-heading" id="panel-heading-682814-1"><h4 class="panel-title"><button class="collapsed"
                                            type="button"
                                            data-toggle="collapse"
                                            data-parent="#accordion-682814"
                                            data-target="#panel-682814-1"
                                            aria-controls="panel-682814-1"
                                            aria-expanded="false"
                                    >
                                        F. Di Salvo, S. Doerrich, I. Rieger, C. Ledig, "An Embedding is Worth a Thousand Noisy Labels," TMLR 2025
                                    </button></h4></div><div id="panel-682814-1"
                                 class="panel-collapse  collapse"
                                 aria-labelledby="panel-heading-682814-1"><div class="panel-body"><div id="c690150" class="  
"><div class="ce-textpic ce-right ce-intext ce-nowrap"><div class="[ ce-gallery  ]" data-ce-columns="1" data-ce-images="1"><div class="ce-gallery__content"><div class="ce-column"><figure data-ce-type="png"><div class="ce-media-copyright-wrapper"><et-copyright copyright-position="bottom-right"><a href="/fileadmin/_processed_/1/d/csm_2025_disalvo_tmlr_figure_63a0babc5c.webp" data-fancybox="gallery690150" data-caption="" aria-label="F. Di Salvo, S. Doerrich, I. Rieger, C. Ledig, &quot;An Embedding is Worth a Thousand Noisy Labels,&quot; TMLR 2025 Gallerie öffnen" data-media="(max-width: 520px);(max-width: 767px);(max-width: 1199px);(max-width: 1399px);(min-width: 1400px)" data-sources="/fileadmin/_processed_/1/d/csm_2025_disalvo_tmlr_figure_51d087ff6d.webp;
                            /fileadmin/_processed_/1/d/csm_2025_disalvo_tmlr_figure_dd7f58e5c0.webp;
                            /fileadmin/_processed_/1/d/csm_2025_disalvo_tmlr_figure_6b474492e8.webp;
                            /fileadmin/_processed_/1/d/csm_2025_disalvo_tmlr_figure_de7669cb19.webp;
                        
                        /fileadmin/_processed_/1/d/csm_2025_disalvo_tmlr_figure_b4ebdd208c.webp" class="fancybox-popup"><picture><source srcset="/fileadmin/_processed_/1/d/csm_2025_disalvo_tmlr_figure_d59896426a.webp" media="(min-width: 981px)" type="image/webp"><source srcset="
                                
                                /fileadmin/_processed_/1/d/csm_2025_disalvo_tmlr_figure_8e5ac1f3ab.webp 474w,
                                /fileadmin/_processed_/1/d/csm_2025_disalvo_tmlr_figure_bf955611d9.webp 274w,
                                /fileadmin/_processed_/1/d/csm_2025_disalvo_tmlr_figure_7495de2da9.webp 200w"
                        sizes="calc(100vw - 30px)"
                        type="image/webp"><img data-cr="xAILab Bamberg" class="image-embed-item " loading="lazy" src="/fileadmin/_processed_/1/d/csm_2025_disalvo_tmlr_figure_68e7817ec1.webp" width="570" height="188" alt="" /></picture></a><span slot="copyright-text" class="ce-copyright">xAILab Bamberg</span></et-copyright></div></figure></div></div></div><div class="ce-bodytext"><h2 class="">
	        
                F. Di Salvo, S. Doerrich, I. Rieger, C. Ledig, "An Embedding is Worth a Thousand Noisy Labels," TMLR 2025
	        
	        </h2><p>Die Leistung tiefer neuronaler Netze skaliert mit der Größe des Datensatzes und der Qualität der Labels. Daher ist eine effiziente Reduzierung von Datenannotationen geringer Qualität für den Aufbau robuster und kosteneffizienter Systeme von entscheidender Bedeutung. Aktuelle Ansätze zur Reduktion von Label-Rauschen sind aufgrund ihrer hohen rechnerischen Komplexität und spezifischen Anwendungsanforderungen stark begrenzt. In dieser Arbeit präsentieren wir WANN, einen gewichteten adaptiven Nearest-Neighbor-Ansatz, der selbstüberwachte Merkmalsdarstellungen verwendet, die aus Basis-Modellen abgeleitet werden. Zur Steuerung des gewichteten Abstimmungsschemas führen wir einen Zuverlässigkeitswert ein. Dieser misst die Wahrscheinlichkeit, dass ein Datenlabel korrekt ist. WANN übertrifft Referenzmethoden – einschließlich einer linearen Schicht, die mit robusten Verlustfunktionen trainiert wurde – bei verschiedenen Datensätzen unterschiedlicher Größe sowie unter verschiedenen Arten und Schweregraden von Rauschen. WANN zeigt auch eine überlegene Generalisierung bei unausgewogenen Daten im Vergleich zu adaptiven neuronalen Netzen (ANN) und festen k-NNs. Darüber hinaus verbessert das vorgeschlagene Gewichtungsschema die überwachte Dimensionsreduktion bei verrauschten Labels. Dies führt zu einer deutlichen Steigerung der Klassifizierungsleistung bei 10-mal und 100-mal kleineren Bild-Embeddings, wodurch sich Latenz und Speicheranforderungen minimieren lassen. Unser Ansatz, der Effizienz und Erklärbarkeit in den Vordergrund stellt, erweist sich somit als einfache und robuste Lösung zur Überwindung der inhärenten Einschränkungen des Trainings tiefer neuronaler Netze.</p><p>Autoren: <a href="https://francescodisalvo05.github.io" target="_blank" rel="noreferrer">Francesco Di Salvo</a>, <a href="https://scholar.google.com/citations?user=9fKkrCcAAAAJ&amp;hl=de" target="_blank" rel="noreferrer">Sebastian Doerrich</a>, <a href="https://scholar.google.com/citations?user=J6qOfR4AAAAJ&amp;hl=en" target="_blank" rel="noreferrer">Ines Rieger</a>, <a href="http://www.christianledig.com/" target="_blank" rel="noreferrer">Christian Ledig</a></p><p><a href="https://arxiv.org/pdf/2408.14358" target="_blank" rel="noreferrer">[Preprint]</a>, <a href="https://openreview.net/forum?id=X3gSvQjShh" target="_blank" rel="noreferrer">[Publication]</a>,<a href="https://github.com/francescodisalvo05/wann-noisy-labels" target="_blank" rel="noreferrer"> [Code]</a>, <a href="/fileadmin/xai/research/disalvo_tmlr2025.bib">[Bibtex]</a><span class="filesize">(309.0 B)</span></p></div></div></div></div></div></div><div class="panel  panel-default"><div class="panel-heading" id="panel-heading-682814-2"><h4 class="panel-title"><button class="collapsed"
                                            type="button"
                                            data-toggle="collapse"
                                            data-parent="#accordion-682814"
                                            data-target="#panel-682814-2"
                                            aria-controls="panel-682814-2"
                                            aria-expanded="false"
                                    >
                                        S. Doerrich, F. Di Salvo, J. Brockmann, C. Ledig, “Rethinking model prototyping through the MedMNIST+ dataset collection”, Scientific Reports, 15, 7669, 2025
                                    </button></h4></div><div id="panel-682814-2"
                                 class="panel-collapse  collapse"
                                 aria-labelledby="panel-heading-682814-2"><div class="panel-body"><div id="c682822" class="  
"><div class="ce-textpic ce-right ce-intext ce-nowrap"><div class="[ ce-gallery  ]" data-ce-columns="1" data-ce-images="1"><div class="ce-gallery__content"><div class="ce-column"><figure data-ce-type="png"><div class="ce-media-copyright-wrapper"><et-copyright copyright-position="bottom-right"><a href="/fileadmin/_processed_/9/a/csm_2025_doerrich_rethinking-model-prototyping-MedMNISTPlus_74adb1ce20.webp" data-fancybox="gallery682822" data-caption="" aria-label="S. Doerrich, F. Di Salvo, J. Brockmann, C. Ledig, “Rethinking model prototyping through the MedMNIST+ dataset collection”, Scientific Reports, 15, 7669, 2025 Gallerie öffnen" data-media="(max-width: 520px);(max-width: 767px);(max-width: 1199px);(max-width: 1399px);(min-width: 1400px)" data-sources="/fileadmin/_processed_/9/a/csm_2025_doerrich_rethinking-model-prototyping-MedMNISTPlus_b2cc0e38b1.webp;
                            /fileadmin/_processed_/9/a/csm_2025_doerrich_rethinking-model-prototyping-MedMNISTPlus_5208b21622.webp;
                            /fileadmin/_processed_/9/a/csm_2025_doerrich_rethinking-model-prototyping-MedMNISTPlus_1423aae47a.webp;
                            /fileadmin/_processed_/9/a/csm_2025_doerrich_rethinking-model-prototyping-MedMNISTPlus_7b699bdb9c.webp;
                        
                        /fileadmin/_processed_/9/a/csm_2025_doerrich_rethinking-model-prototyping-MedMNISTPlus_3ad967e563.webp" class="fancybox-popup"><picture><source srcset="/fileadmin/_processed_/9/a/csm_2025_doerrich_rethinking-model-prototyping-MedMNISTPlus_faa831386c.webp" media="(min-width: 981px)" type="image/webp"><source srcset="
                                
                                /fileadmin/_processed_/9/a/csm_2025_doerrich_rethinking-model-prototyping-MedMNISTPlus_f8f2eb76e8.webp 474w,
                                /fileadmin/_processed_/9/a/csm_2025_doerrich_rethinking-model-prototyping-MedMNISTPlus_c2034b6e0d.webp 274w,
                                /fileadmin/_processed_/9/a/csm_2025_doerrich_rethinking-model-prototyping-MedMNISTPlus_5da8912f22.webp 200w"
                        sizes="calc(100vw - 30px)"
                        type="image/webp"><img data-cr="xAILab Bamberg" class="image-embed-item " loading="lazy" src="/fileadmin/_processed_/9/a/csm_2025_doerrich_rethinking-model-prototyping-MedMNISTPlus_cd07e66eee.webp" width="570" height="331" alt="" /></picture></a><span slot="copyright-text" class="ce-copyright">xAILab Bamberg</span></et-copyright></div></figure></div></div></div><div class="ce-bodytext"><h2 class="">
	        
                S. Doerrich, F. Di Salvo, J. Brockmann, C. Ledig, “Rethinking model prototyping through the MedMNIST+ dataset collection”, Scientific Reports, 15, 7669, 2025
	        
	        </h2><p>Die Integration von auf Deep Learning basierenden Systemen in die klinische Praxis wird häufig durch Herausforderungen behindert, die in begrenzten und heterogenen medizinischen Datensätzen begründet sind. Darüber hinaus wird in diesem Bereich zunehmend marginalen Leistungssteigerungen bei einigen wenigen, eng gefassten Benchmarks Vorrang vor der klinischen Anwendbarkeit eingeräumt, wodurch sinnvolle algorithmische Fortschritte verlangsamt werden. Dieser Trend führt häufig zu einer übermäßigen Feinabstimmung bestehender Methoden an ausgewählten Datensätzen, anstatt klinisch relevante Innovationen zu fördern. Als Reaktion darauf wird in dieser Arbeit ein umfassender Benchmark für die MedMNIST+-Datensatzsammlung eingeführt, der die Bewertungslandschaft über mehrere Bildgebungsmodalitäten, anatomische Regionen, Klassifizierungsaufgaben und Stichprobengrößen hinweg diversifizieren soll. Wir bewerten systematisch häufig verwendete Convolutional Neural Networks (CNNs) und Vision Transformer (ViT)-Architekturen in verschiedenen medizinischen Datensätzen, Trainingsmethoden und Eingabeauflösungen, um bestehende Annahmen über die Effektivität und Entwicklung von Modellen zu validieren und zu verfeinern. Unsere Ergebnisse deuten darauf hin, dass rechnerisch effiziente Trainingsverfahren und moderne Foundation-modelle praktikable Alternativen zum kostspieligen End-to-End-Training bieten. Darüber hinaus stellen wir fest, dass höhere Bildauflösungen die Leistung ab einem bestimmten Schwellenwert nicht durchgängig verbessern. Dies unterstreicht die potenziellen Vorteile der Verwendung niedrigerer Auflösungen, insbesondere in der Prototyping-Phase, um den Rechenaufwand zu verringern, ohne die Genauigkeit zu beeinträchtigen. Insbesondere bestätigt unsere Analyse die Wettbewerbsfähigkeit von CNNs im Vergleich zu ViTs und unterstreicht, wie wichtig es ist, die intrinsischen Fähigkeiten der verschiedenen Architekturen zu verstehen. Schließlich wollen wir durch die Schaffung eines standardisierten Bewertungsrahmens die Transparenz, Reproduzierbarkeit und Vergleichbarkeit innerhalb der MedMNIST+-Datensatzsammlung sowie die zukünftige Forschung verbessern.</p><p>Autoren: <a href="https://scholar.google.com/citations?user=9fKkrCcAAAAJ&amp;hl=de" target="_blank" rel="noreferrer">Sebastian Doerrich</a>, <a href="https://francescodisalvo05.github.io/" target="_blank" rel="noreferrer">Francesco Di Salvo</a>, Julius Brockmann, <a href="http://www.christianledig.com/" target="_blank" rel="noreferrer">Christian Ledig</a></p><p><a href="https://arxiv.org/abs/2404.15786" target="_blank" rel="noreferrer">[Preprint]</a>, <a href="https://www.nature.com/articles/s41598-025-92156-9" target="_blank" rel="noreferrer">[Publication]</a>, <a href="https://github.com/sdoerrich97/rethinking-model-prototyping-MedMNISTPlus" target="_blank" rel="noreferrer">[Code]</a>, <a href="https://github.com/sdoerrich97/rethinking-model-prototyping-MedMNISTPlus/blob/main/BENCHMARK.md" target="_blank" rel="noreferrer">[Benchmark]</a>, <a href="/fileadmin/xai/research/2024_doerrich_privacy-aware-image-classification-with-kNN.bib">[BibTeX]</a><span class="filesize">(612.0 B)</span></p></div></div></div></div></div></div><div class="panel  panel-default"><div class="panel-heading" id="panel-heading-682814-3"><h4 class="panel-title"><button class="collapsed"
                                            type="button"
                                            data-toggle="collapse"
                                            data-parent="#accordion-682814"
                                            data-target="#panel-682814-3"
                                            aria-controls="panel-682814-3"
                                            aria-expanded="false"
                                    >
                                        F. Di Salvo, D. Tafler, S. Doerrich, C. Ledig, "Privacy-preserving datasets by capturing feature distributions with Conditional VAEs," BMVC 2024
                                    </button></h4></div><div id="panel-682814-3"
                                 class="panel-collapse  collapse"
                                 aria-labelledby="panel-heading-682814-3"><div class="panel-body"><div id="c690127" class="  
"><div class="ce-textpic ce-right ce-intext ce-nowrap"><div class="[ ce-gallery  ]" data-ce-columns="1" data-ce-images="1"><div class="ce-gallery__content"><div class="ce-column"><figure data-ce-type="pdf"><div class="ce-media-copyright-wrapper"><et-copyright copyright-position="bottom-right"><a href="/fileadmin/_processed_/7/8/csm_2024_bmvc_disalvo_8839c575e2.webp" data-fancybox="gallery690127" data-caption="" aria-label="F. Di Salvo, D. Tafler, S. Doerrich, C. Ledig, &quot;Privacy-preserving datasets by capturing feature distributions with Conditional VAEs,&quot; BMVC 2024 Gallerie öffnen" data-media="(max-width: 520px);(max-width: 767px);(max-width: 1199px);(max-width: 1399px);(min-width: 1400px)" data-sources="/fileadmin/_processed_/7/8/csm_2024_bmvc_disalvo_bec5dda59a.webp;
                            /fileadmin/_processed_/7/8/csm_2024_bmvc_disalvo_573b90fb82.webp;
                            /fileadmin/_processed_/7/8/csm_2024_bmvc_disalvo_39fb53e688.webp;
                            /fileadmin/_processed_/7/8/csm_2024_bmvc_disalvo_0d9328ba24.webp;
                        
                        /fileadmin/_processed_/7/8/csm_2024_bmvc_disalvo_a3e2cb4739.webp" class="fancybox-popup"><picture><source srcset="/fileadmin/_processed_/7/8/csm_2024_bmvc_disalvo_4c40d41480.webp" media="(min-width: 981px)" type="image/webp"><source srcset="
                                
                                /fileadmin/_processed_/7/8/csm_2024_bmvc_disalvo_ee6360ec69.webp 474w,
                                /fileadmin/_processed_/7/8/csm_2024_bmvc_disalvo_9e22b7fbef.webp 274w,
                                /fileadmin/_processed_/7/8/csm_2024_bmvc_disalvo_e6d8d06b2c.webp 200w"
                        sizes="calc(100vw - 30px)"
                        type="image/webp"><img data-cr="xAILab Bamberg" class="image-embed-item " loading="lazy" src="/fileadmin/_processed_/7/8/csm_2024_bmvc_disalvo_725967927d.webp" width="570" height="805" alt="" /></picture></a><span slot="copyright-text" class="ce-copyright">xAILab Bamberg</span></et-copyright></div></figure></div></div></div><div class="ce-bodytext"><h2 class="">
	        
                F. Di Salvo, D. Tafler, S. Doerrich, C. Ledig, "Privacy-preserving datasets by capturing feature distributions with Conditional VAEs," BMVC 2024
	        
	        </h2><p>Für die Weiterentwicklung von Deep-Learning-Anwendungen sind große und gut annotierte Datensätze unerlässlich. Diese sind jedoch oft kostspielig oder für eine einzelne Einrichtung unmöglich zu beschaffen. In vielen Bereichen, darunter auch im medizinischen Bereich, haben sich Ansätze durchgesetzt, die auf Datenaustausch basieren, um diese Herausforderungen zu bewältigen. Datenaustausch ist zwar wirksam, um die Größe und Vielfalt von Datensätzen zu erhöhen, er wirft jedoch auch erhebliche Datenschutzbedenken auf. Häufig verwendete Anonymisierungsmethoden, die auf dem k-Anonymitätsparadigma basieren, können die Datenvielfalt oft nicht bewahren, wodurch die Robustheit des Modells beeinträchtigt wird. In dieser Arbeit wird ein neuartiger Ansatz vorgestellt, der Conditional Variational Autoencoders (CVAEs) verwendet. Diese wurden auf Merkmalsvektoren trainiert, die aus großen, vorab trainierten Vision-Foundation-Modellen extrahiert wurden. Foundation-Modelle erkennen und repräsentieren effektiv komplexe Muster über verschiedene Domänen hinweg. Dadurch kann der CVAE den Einbettungsraum einer bestimmten Datenverteilung originalgetreu erfassen, um eine vielfältige, datenschutzkonforme und potenziell unbegrenzte Menge synthetischer Merkmalsvektoren zu generieren (zu samplen). Unsere Methode übertrifft traditionelle Ansätze sowohl im medizinischen als auch im natürlichen Bildbereich deutlich. Sie weist eine größere Datensatzvielfalt und eine höhere Robustheit gegenüber Störungen auf, während die Privatsphäre der Stichproben gewahrt bleibt. Diese Ergebnisse unterstreichen das Potenzial generativer Modelle, Deep-Learning-Anwendungen in datenarmen und datenschutzsensiblen Umgebungen erheblich zu verbessern.</p><p>Autoren: <a href="https://francescodisalvo05.github.io" target="_blank" rel="noreferrer">Francesco Di Salvo</a>, <a href="https://ch.linkedin.com/in/davidtafler" target="_blank" rel="noreferrer">David Tafler</a>, <a href="https://scholar.google.com/citations?user=9fKkrCcAAAAJ&amp;hl=de" target="_blank" rel="noreferrer">Sebastian Doerrich</a>, <a href="http://www.christianledig.com/" target="_blank" rel="noreferrer">Christian Ledig</a></p><p><a href="https://arxiv.org/pdf/2408.00639" target="_blank" rel="noreferrer">[Preprint]</a>, <a href="https://bmvc2024.org/proceedings/145/" target="_blank" rel="noreferrer">[Publication]</a>, <a href="https://github.com/francescodisalvo05/cvae-anonymization" target="_blank" rel="noreferrer">[Code]</a>, <a href="/fileadmin/xai/research/disalvo_bmvc2024.bib">[Bibtex]</a><span class="filesize">(425.0 B)</span></p></div></div></div></div></div></div><div class="panel  panel-default"><div class="panel-heading" id="panel-heading-682814-4"><h4 class="panel-title"><button class="collapsed"
                                            type="button"
                                            data-toggle="collapse"
                                            data-parent="#accordion-682814"
                                            data-target="#panel-682814-4"
                                            aria-controls="panel-682814-4"
                                            aria-expanded="false"
                                    >
                                        F. Di Salvo, S. Doerrich, C. Ledig, "MedMNIST-C: Comprehensive benchmark and improved classifier robustness by simulating realistic image corruptions," ADSMI @ MICCAI 2025
                                    </button></h4></div><div id="panel-682814-4"
                                 class="panel-collapse  collapse"
                                 aria-labelledby="panel-heading-682814-4"><div class="panel-body"><div id="c688785" class="  
"><div class="ce-textpic ce-right ce-intext ce-nowrap"><div class="[ ce-gallery  ]" data-ce-columns="1" data-ce-images="1"><div class="ce-gallery__content"><div class="ce-column"><figure data-ce-type="pdf"><div class="ce-media-copyright-wrapper"><et-copyright copyright-position="bottom-right"><a href="/fileadmin/_processed_/3/3/csm_2024_disalvo_adsmi_v4_16fb3aff4b.webp" data-fancybox="gallery688785" data-caption="" aria-label="F. Di Salvo, S. Doerrich, C. Ledig, &quot;MedMNIST-C: Comprehensive benchmark and improved classifier robustness by simulating realistic image corruptions,&quot; ADSMI @ MICCAI 2025 Gallerie öffnen" data-media="(max-width: 520px);(max-width: 767px);(max-width: 1199px);(max-width: 1399px);(min-width: 1400px)" data-sources="/fileadmin/_processed_/3/3/csm_2024_disalvo_adsmi_v4_4862e65036.webp;
                            /fileadmin/_processed_/3/3/csm_2024_disalvo_adsmi_v4_00f8015c3d.webp;
                            /fileadmin/_processed_/3/3/csm_2024_disalvo_adsmi_v4_cf2a4840c5.webp;
                            /fileadmin/_processed_/3/3/csm_2024_disalvo_adsmi_v4_1dcf396b6e.webp;
                        
                        /fileadmin/_processed_/3/3/csm_2024_disalvo_adsmi_v4_7af4e585f5.webp" class="fancybox-popup"><picture><source srcset="/fileadmin/_processed_/3/3/csm_2024_disalvo_adsmi_v4_8aac3571d0.webp" media="(min-width: 981px)" type="image/webp"><source srcset="
                                
                                /fileadmin/_processed_/3/3/csm_2024_disalvo_adsmi_v4_70125ceb13.webp 474w,
                                /fileadmin/_processed_/3/3/csm_2024_disalvo_adsmi_v4_c0978ee781.webp 274w,
                                /fileadmin/_processed_/3/3/csm_2024_disalvo_adsmi_v4_62b640afa0.webp 200w"
                        sizes="calc(100vw - 30px)"
                        type="image/webp"><img data-cr="xAILab Bamberg" class="image-embed-item " loading="lazy" src="/fileadmin/_processed_/3/3/csm_2024_disalvo_adsmi_v4_05e4196360.webp" width="570" height="805" alt="" /></picture></a><span slot="copyright-text" class="ce-copyright">xAILab Bamberg</span></et-copyright></div></figure></div></div></div><div class="ce-bodytext"><h2 class="">
	        
                F. Di Salvo, S. Doerrich, C. Ledig, "MedMNIST-C: Comprehensive benchmark and improved classifier robustness by simulating realistic image corruptions," ADSMI @ MICCAI 2025
	        
	        </h2><p>Die Integration von Systemen auf Basis neuronaler Netze in die klinische Praxis ist aufgrund von Herausforderungen im Zusammenhang mit Domänengeneralisierung und Robustheit eingeschränkt. Die Computer-Vision-Community hat Benchmarks wie ImageNet-C als grundlegende Voraussetzung für die Messung der Fortschritte bei der Bewältigung dieser Herausforderungen etabliert. In der medizinischen Bildgebungsgemeinschaft fehlen ähnliche Datensätze weitgehend, da es an einem umfassenden Benchmark mangelt, der alle Bildgebungsmodalitäten und -anwendungen abdeckt. Um diese Lücke zu schließen, haben wir den Benchmark-Datensatz MedMNIST-C erstellt und als Open Source veröffentlicht. Er basiert auf der MedMNIST+-Sammlung und umfasst 12 Datensätze und 9 Bildgebungsmodalitäten. Mithilfe von simulierten, aufgaben- und modalitätsspezifischen Bildverfälschungen unterschiedlicher Schwere bewerten wir die Robustheit etablierter Algorithmen gegenüber realen Artefakten und Verteilungsverschiebungen umfassend. Darüber hinaus liefern wir quantitative Belege dafür, dass unsere leicht zu verwendenden künstlichen Verfälschungen eine leistungsfähige und effiziente Datenvergrößerung ermöglichen und somit die Robustheit des Modells verbessern. Im Gegensatz zu traditionellen, generischen Vergrößerungsstrategien nutzt unser Ansatz Domänenwissen und ist im Vergleich zu weit verbreiteten Methoden deutlich robuster. Mit der Einführung von MedMNIST-C und der Veröffentlichung der entsprechenden Bibliothek, die gezielte Datenvergrößerungen ermöglicht, tragen wir zur Entwicklung immer robusterer, auf die Herausforderungen der medizinischen Bildgebung zugeschnittener Methoden bei.</p><p>Autoren: <a href="https://francescodisalvo05.github.io/" target="_blank" rel="noreferrer">Francesco Di Salvo</a>, <a href="https://scholar.google.com/citations?user=9fKkrCcAAAAJ&amp;hl=de" target="_blank" rel="noreferrer">Sebastian Doerrich</a>, <a href="http://www.christianledig.com/" target="_blank" rel="noreferrer">Christian Ledig</a></p><p><a href="https://arxiv.org/pdf/2406.17536" target="_blank" rel="noreferrer">[preprint]</a>, <a href="https://github.com/francescodisalvo05/medmnistc-api" target="_blank" rel="noreferrer">[code],</a><a href="https://zenodo.org/records/11471504" target="_blank" rel="noreferrer">[dataset]</a>, <a href="/fileadmin/xai/research/2024_disalvo_adsmi2024.bib">[bibtex]</a><span class="filesize">(291.0 B)</span></p></div></div></div></div></div></div><div class="panel  panel-default"><div class="panel-heading" id="panel-heading-682814-5"><h4 class="panel-title"><button class="collapsed"
                                            type="button"
                                            data-toggle="collapse"
                                            data-parent="#accordion-682814"
                                            data-target="#panel-682814-5"
                                            aria-controls="panel-682814-5"
                                            aria-expanded="false"
                                    >
                                        S. Doerrich, F. Di Salvo, C. Ledig, "Self-supervised Vision Transformer are Scalable Generative Models for Domain Generalization", MICCAI, 2024
                                    </button></h4></div><div id="panel-682814-5"
                                 class="panel-collapse  collapse"
                                 aria-labelledby="panel-heading-682814-5"><div class="panel-body"><div id="c682818" class="  
"><div class="ce-textpic ce-right ce-intext ce-nowrap"><div class="[ ce-gallery  ]" data-ce-columns="1" data-ce-images="1"><div class="ce-gallery__content"><div class="ce-column"><figure data-ce-type="pdf"><div class="ce-media-copyright-wrapper"><et-copyright copyright-position="bottom-right"><a href="/fileadmin/_processed_/c/d/csm_2024_doerrich_vits-are-generative-models_0d9161e0b4.webp" data-fancybox="gallery682818" data-caption="" aria-label="S. Doerrich, F. Di Salvo, C. Ledig, &quot;Self-supervised Vision Transformer are Scalable Generative Models for Domain Generalization&quot;, MICCAI, 2024 Gallerie öffnen" data-media="(max-width: 520px);(max-width: 767px);(max-width: 1199px);(max-width: 1399px);(min-width: 1400px)" data-sources="/fileadmin/_processed_/c/d/csm_2024_doerrich_vits-are-generative-models_d7067b2446.webp;
                            /fileadmin/_processed_/c/d/csm_2024_doerrich_vits-are-generative-models_1604cb0948.webp;
                            /fileadmin/_processed_/c/d/csm_2024_doerrich_vits-are-generative-models_5140cf89e7.webp;
                            /fileadmin/_processed_/c/d/csm_2024_doerrich_vits-are-generative-models_4b9840978b.webp;
                        
                        /fileadmin/_processed_/c/d/csm_2024_doerrich_vits-are-generative-models_d6ae6848a5.webp" class="fancybox-popup"><picture><source srcset="/fileadmin/_processed_/c/d/csm_2024_doerrich_vits-are-generative-models_78b44b1bef.webp" media="(min-width: 981px)" type="image/webp"><source srcset="
                                
                                /fileadmin/_processed_/c/d/csm_2024_doerrich_vits-are-generative-models_e91d10484d.webp 474w,
                                /fileadmin/_processed_/c/d/csm_2024_doerrich_vits-are-generative-models_fc63f9f1f7.webp 274w,
                                /fileadmin/_processed_/c/d/csm_2024_doerrich_vits-are-generative-models_4dc0e58848.webp 200w"
                        sizes="calc(100vw - 30px)"
                        type="image/webp"><img data-cr="xAILab Bamberg" class="image-embed-item " loading="lazy" src="/fileadmin/_processed_/c/d/csm_2024_doerrich_vits-are-generative-models_e1291d4130.webp" width="570" height="805" alt="" /></picture></a><span slot="copyright-text" class="ce-copyright">xAILab Bamberg</span></et-copyright></div></figure></div></div></div><div class="ce-bodytext"><h2 class="">
	        
                S. Doerrich, F. Di Salvo, C. Ledig, "Self-supervised Vision Transformer are Scalable Generative Models for Domain Generalization", MICCAI, 2024
	        
	        </h2><p>Trotz bemerkenswerter Fortschritte wurde die Integration von Deep Learning (DL)-Techniken in wirkungsvolle klinische Anwendungen, insbesondere im Bereich der digitalen Histopathologie, durch die Herausforderungen behindert, die mit dem Erreichen einer robusten Generalisierung über verschiedene Bildgebungsdomänen und -merkmale verbunden sind. Traditionelle Strategien in diesem Bereich wie Datenerweiterung und Fleckenfarbnormalisierung haben sich als unzureichend erwiesen, um diese Einschränkung zu beheben, was die Erforschung alternativer Methoden erforderlich machte. Zu diesem Zweck schlagen wir eine neuartige generative Methode zur Generalisierung von Histopathologiebildern vor. Unsere Methode verwendet einen generativen, selbstüberwachten Vision Transformer, um dynamisch Merkmale von Bildfeldern zu extrahieren und sie nahtlos in die Originalbilder einzufügen, wodurch neue, synthetische Bilder mit verschiedenen Attributen entstehen. Indem wir den Datensatz mit solchen synthetischen Bildern anreichern, wollen wir seine Ganzheitlichkeit erhöhen und eine verbesserte Generalisierung von DL-Modellen auf unbekannte Bereiche ermöglichen. Ausführliche Experimente mit zwei verschiedenen Histopathologiedatensätzen zeigen die Effektivität des von uns vorgeschlagenen Ansatzes, der den Stand der Technik deutlich übertrifft, und zwar beim Camelyon17-Wilds-Datensatz (+2%) und bei einem zweiten Epithelium-Stroma-Datensatz (+26%). Darüber hinaus heben wir die Fähigkeit unserer Methode hervor, mit zunehmend verfügbaren unbeschrifteten Datenproben und komplexeren, höher parametrischen Architekturen zu skalieren.</p><p>Autoren: <a href="https://scholar.google.com/citations?user=9fKkrCcAAAAJ&amp;hl=de" target="_blank" rel="noreferrer">Sebastian Doerrich</a>, <a href="https://francescodisalvo05.github.io/" target="_blank" rel="noreferrer">Francesco Di Salvo</a>, <a href="http://www.christianledig.com/" target="_blank" rel="noreferrer">Christian Ledig</a></p><p><a href="https://arxiv.org/abs/2407.02900" target="_blank" rel="noreferrer">[Preprint]</a>, <a href="https://link.springer.com/chapter/10.1007/978-3-031-72117-5_60" target="_blank" rel="noreferrer">[Publication]</a>, <a href="https://github.com/sdoerrich97/vits-are-generative-models" target="_blank" rel="noreferrer">[Code]</a>, <a href="/fileadmin/xai/research/2024_doerrich_privacy-aware-image-classification-with-kNN.bib">[BibTeX]</a><span class="filesize">(612.0 B)</span></p></div></div></div></div></div></div><div class="panel  panel-default"><div class="panel-heading" id="panel-heading-682814-6"><h4 class="panel-title"><button class="collapsed"
                                            type="button"
                                            data-toggle="collapse"
                                            data-parent="#accordion-682814"
                                            data-target="#panel-682814-6"
                                            aria-controls="panel-682814-6"
                                            aria-expanded="false"
                                    >
                                        S. Doerrich, T. Archut, F. Di Salvo, C. Ledig, "Integrating kNN with Foundation Models for Adaptable and Privacy-Aware Image Classification", IEEE ISBI, 2024
                                    </button></h4></div><div id="panel-682814-6"
                                 class="panel-collapse  collapse"
                                 aria-labelledby="panel-heading-682814-6"><div class="panel-body"><div id="c682816" class="  
"><div class="ce-textpic ce-right ce-intext ce-nowrap"><div class="[ ce-gallery  ]" data-ce-columns="1" data-ce-images="1"><div class="ce-gallery__content"><div class="ce-column"><figure data-ce-type="pdf"><div class="ce-media-copyright-wrapper"><et-copyright copyright-position="bottom-right"><a href="/fileadmin/_processed_/7/a/csm_2024_doerrich_privacy-aware-image-classification-with-kNN_b4821cb42c.webp" data-fancybox="gallery682816" data-caption="" aria-label="S. Doerrich, T. Archut, F. Di Salvo, C. Ledig, &quot;Integrating kNN with Foundation Models for Adaptable and Privacy-Aware Image Classification&quot;, IEEE ISBI, 2024 Gallerie öffnen" data-media="(max-width: 520px);(max-width: 767px);(max-width: 1199px);(max-width: 1399px);(min-width: 1400px)" data-sources="/fileadmin/_processed_/7/a/csm_2024_doerrich_privacy-aware-image-classification-with-kNN_1a4a405ac8.webp;
                            /fileadmin/_processed_/7/a/csm_2024_doerrich_privacy-aware-image-classification-with-kNN_c7688b4fbb.webp;
                            /fileadmin/_processed_/7/a/csm_2024_doerrich_privacy-aware-image-classification-with-kNN_13b506fa9b.webp;
                            /fileadmin/_processed_/7/a/csm_2024_doerrich_privacy-aware-image-classification-with-kNN_9bc84b1c8e.webp;
                        
                        /fileadmin/_processed_/7/a/csm_2024_doerrich_privacy-aware-image-classification-with-kNN_9195e9ae4d.webp" class="fancybox-popup"><picture><source srcset="/fileadmin/_processed_/7/a/csm_2024_doerrich_privacy-aware-image-classification-with-kNN_17eea96adf.webp" media="(min-width: 981px)" type="image/webp"><source srcset="
                                
                                /fileadmin/_processed_/7/a/csm_2024_doerrich_privacy-aware-image-classification-with-kNN_6220f337d9.webp 474w,
                                /fileadmin/_processed_/7/a/csm_2024_doerrich_privacy-aware-image-classification-with-kNN_347305b1df.webp 274w,
                                /fileadmin/_processed_/7/a/csm_2024_doerrich_privacy-aware-image-classification-with-kNN_cc8a0f20bb.webp 200w"
                        sizes="calc(100vw - 30px)"
                        type="image/webp"><img data-cr="xAILab Bamberg" class="image-embed-item " loading="lazy" src="/fileadmin/_processed_/7/a/csm_2024_doerrich_privacy-aware-image-classification-with-kNN_957bd9af13.webp" width="570" height="805" alt="" /></picture></a><span slot="copyright-text" class="ce-copyright">xAILab Bamberg</span></et-copyright></div></figure></div></div></div><div class="ce-bodytext"><h2 class="">
	        
                S. Doerrich, T. Archut, F. Di Salvo, C. Ledig, "Integrating kNN with Foundation Models for Adaptable and Privacy-Aware Image Classification", IEEE ISBI, 2024
	        
	        </h2><p>Herkömmliche Deep-Learning-Modelle kodieren Wissen implizit, was ihre Transparenz und ihre Fähigkeit zur Anpassung an Datenänderungen einschränkt. Diese Anpassungsfähigkeit ist jedoch entscheidend für den Schutz der Daten der Nutzer. Wir beheben diese Einschränkung, indem wir die Einbettungen der zugrunde liegenden Trainingsdaten unabhängig von den Modellgewichten speichern und so dynamische Datenänderungen ohne erneutes Training ermöglichen. Unser Ansatz integriert insbesondere den k-Nearest-Neighbor-Klassifikator (k-NN) mit einem bildbasierten Foundation-modell, das selbstüberwacht auf natürlichen Bildern trainiert wurde, was die Interpretierbarkeit und Anpassungsfähigkeit verbessert. Wir stellen Open-Source-Implementierungen einer bisher unveröffentlichten Basismethode sowie unsere leistungssteigernden Beiträge zur Verfügung. Quantitative Experimente bestätigen die verbesserte Klassifikation in etablierten Benchmark-Datensätzen und die Anwendbarkeit der Methode auf verschiedene medizinische Bildklassifikationsaufgaben. Darüber hinaus bewerten wir die Robustheit der Methode in Szenarien mit kontinuierlichem Lernen und Datenentfernung. Der Ansatz ist sehr vielversprechend, um die Lücke zwischen der Leistung von Foundation-modellen und den Herausforderungen des Datenschutzes zu schließen.</p><p>Autoren: <a href="https://scholar.google.com/citations?user=9fKkrCcAAAAJ&amp;hl=de" target="_blank" rel="noreferrer">Sebastian Doerrich</a>, Tobias Archut, <a href="https://francescodisalvo05.github.io/" target="_blank" rel="noreferrer">Francesco Di Salvo</a>, <a href="http://www.christianledig.com/" target="_blank" rel="noreferrer">Christian Ledig</a></p><p><a href="https://arxiv.org/abs/2402.12500" target="_blank" rel="noreferrer">[Preprint]</a>, <a href="https://ieeexplore.ieee.org/abstract/document/10635560" target="_blank" rel="noreferrer">[Publication]</a>, <a href="https://github.com/TobArc/privacy-aware-image-classification-with-kNN" target="_blank" rel="noreferrer">[Code]</a>, <a href="/fileadmin/xai/research/2024_doerrich_privacy-aware-image-classification-with-kNN.bib">[BibTeX]</a><span class="filesize">(612.0 B)</span></p><p>&nbsp;</p></div></div></div></div></div></div></div></div></section><div class="container  container__totop  js-totop-link  is-pinned"><div class="c-totop"><a class="c-totop_link" href="#top" title="Zum Seitenanfang"></a></div></div></article><!--TYPO3SEARCH_end--></main></div><footer class="page-content__footer  [ stripe  stripe--content-footer ] "><div class="page-content-footer__wrapper "><div class="page-content-footer__inner-wrapper"><div class=""><p class="page-content-footer__item page-content-footer__descr"><span class="seitenid">Seite 150582</span></p><nav class="page-content-footer__item page-content__nav-meta nav-meta"><ul class="[ list-inline ]"><li><a href="/xai/kontaktnavigation/kontakt/">Kontakt</a></li><li><a href="/xai/kontaktnavigation/impressum/">Impressum</a></li><li><a href="/its/verfahrensweisen/datenschutz/datenschutzerklaerungen/webauftritt/">Datenschutz</a></li></ul></nav></div></div></div></footer><footer class="page-footer [ stripe  stripe--footer ]"><div class="container  container--footer  page-footer__container"><div class="page-footer__items"><nav class="page-footer__item  page-footer__nav" aria-labelledby="footer-headline-left"><p class="page-footer__item__headline" id="footer-headline-left">Zentrale Online-Services</p><div><ul><li><a href="https://katalog.ub.uni-bamberg.de/" target="_blank" class="external-link-new-window" rel="noreferrer">Bamberger Katalog (Universitätsbibliothek)</a></li><li><a href="https://fis.uni-bamberg.de/" target="_blank" rel="noreferrer">FIS (Forschungsinformationssystem)</a></li><li><a href="/pruefungsamt/fn2sss/" title="FlexNow 2 Uni Bamberg">FlexNow2 für Studierende</a></li><li><a href="/pruefungsamt/flexnow/fn2web/" title="FlexNow 2 Uni Bamberg">FlexNow2 für Mitarbeitende</a></li><li><a href="/intranet/">Intranet</a></li><li><a href="https://o365.uni-bamberg.de/" target="_blank" rel="noreferrer">Office 365</a></li><li><a href="https://qis.uni-bamberg.de/" target="_blank" rel="noreferrer">Online-Dienste</a><br><a href="https://qis.uni-bamberg.de/" target="_blank" rel="noreferrer">(Studierendenkanzlei)</a></li><li><a href="https://univis.uni-bamberg.de" target="_blank" title="UnivIS Uni Bamberg" rel="noreferrer">UnivIS</a></li><li>Uni-Webmail:<br><a href="https://mailex.uni-bamberg.de/" target="_blank" rel="noreferrer">https://mailex.uni-bamberg.de</a><br><a href="https://o365.uni-bamberg.de/" target="_blank" rel="noreferrer">https://o365.uni-bamberg.de</a></li><li><a href="https://vc.uni-bamberg.de" target="_blank" title="VC Uni Bamberg (Virtueller Campus)" rel="noreferrer">Virtueller Campus</a></li></ul></div></nav><nav class="page-footer__item  page-footer__nav" aria-labelledby="footer-headline-middle"><p class="page-footer__item__headline" id="footer-headline-middle">Wichtige Links</p><div><ul><li><a href="/presse/pressemitteilungen/">Presse</a></li><li><a href="/universitaet/anreise/oeffnungszeiten/">Öffnungszeiten</a></li><li><a href="/pruefungsamt/">Prüfungsamt</a></li><li><a href="/its/">IT-Service der Universität</a></li><li><a href="/studium/im-studium/studienorganisation/rueckmeldung/">Rückmeldung</a></li><li><a href="/studierendenkanzlei/">Studierendenkanzlei</a></li><li><a href="/ub/">Universitätsbibliothek</a></li><li><a href="/studium/im-studium/studienorganisation/vorlesungszeiten/">Vorlesungszeiten &amp; Fristen</a></li><li><a href="/studienangebot/">Studienangebot</a></li></ul></div><div><p><a href="/its/verfahrensweisen/barrierefreiheit-erklaerung/" title="Erklärung zur Barrierefreiheit">Erklärung zur Barrierefreiheit</a></p><p><a href="/leichte-sprache/">Uni Bamberg in leichter Sprache</a></p><p>&nbsp;</p></div></nav><section class="page-footer__item  page-footer__item--address  link-marker-none" aria-labelledby="footer-headline-right"><p class="page-footer__item__headline" id="footer-headline-right">Zentraler Kontakt </p><div><p>Otto-Friedrich-Universität Bamberg<br> Kapuzinerstraße 16<br> 96047 Bamberg</p><p>Telefon: 0951 863-0<br> E-Mail: <a href="#" data-mailto-token="kygjrm8nmqrYslg+zykzcpe,bc" data-mailto-vector="-2">post(at)uni-bamberg.de</a></p><ul><li><a href="/universitaet/charakteristika-und-kultur/netzwerke/">Unsere Netzwerke</a></li><li><a href="/zertifikate/">Unsere Zertifikate</a></li></ul></div><div class="page-footer__socialmedia"><p class="page-footer__item__headline">Folgen Sie uns</p><a class="link--instagram" href="https://www.instagram.com/uni_bamberg/" target="_blank" rel="noreferrer" title="Instagram" aria-label="Instagram">Instagram</a><a class="link--facebook" href="https://www.facebook.com/UniBamberg/" target="_blank" rel="noreferrer" title="Facebook" aria-label="Facebook">Facebook</a><a class="link--bluesky" href="https://bsky.app/profile/uni-bamberg.de" target="_blank" rel="noreferrer" title="Bluesky" aria-label="Bluesky">Bluesky</a><a class="link--tiktok" href="https://www.tiktok.com/@unibamberg" target="_blank" rel="noreferrer" title="Tiktok" aria-label="Tiktok">Toktok</a><br/><a href="/universitaet/aktuelles/newsletter-abonnieren/" title="Newsletter abonnieren" class="link--newsletter">Newsletter abonnieren</a></div></section></div></div></footer><script nonce="-JoiRsgFeENKiL1nfI_B3lZoEHx3qBjm-J3EQfgEHVJwHOxaXQzgwQ">
            var addTrackerId = 34;
            var _paq = _paq || [];
            _paq.push(["disableCookies"]);
            _paq.push(['trackPageView']);
            _paq.push(['enableLinkTracking']);
            _paq.push(['addDownloadExtensions', "html|html"]);
            (function() {
                var u="//matomo.www.uni-bamberg.de/";
                _paq.push(['setTrackerUrl', u+'piwik.php']);
                _paq.push(['setSiteId', 1]);
                if (typeof addTrackerId !== 'undefined') {
                    _paq.push([ 'addTracker', u+'piwik.php', addTrackerId ]);
                }
                var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
                g.type='text/javascript'; g.async=true; g.defer=true; g.src=u+'piwik.js'; s.parentNode.insertBefore(g,s);
            })();
        </script><noscript><img src="//matomo.www.uni-bamberg.de/piwik.php?idsite=1" style="border:0;" alt=""></noscript>




<script id="powermail_conditions_container" data-condition-uri="https://www.uni-bamberg.de/xai/forschung?type=3132"></script>

<script src="/_assets/948410ace0dfa9ad00627133d9ca8a23/JavaScript/Powermail/Form.min.js?1760518844" defer="defer" nonce="-JoiRsgFeENKiL1nfI_B3lZoEHx3qBjm-J3EQfgEHVJwHOxaXQzgwQ"></script>
<script src="/_assets/6b4f2a2766cf7ae23f682cf290eb0950/JavaScript/PowermailCondition.min.js?1763136161" defer="defer" nonce="-JoiRsgFeENKiL1nfI_B3lZoEHx3qBjm-J3EQfgEHVJwHOxaXQzgwQ"></script>
<script src="/_assets/47400bb84e4e0324cd1d0a84db430fe5/JavaScript/decode.js?1741641568" nonce="-JoiRsgFeENKiL1nfI_B3lZoEHx3qBjm-J3EQfgEHVJwHOxaXQzgwQ"></script>
<script type="module" src="/_frontend/bundled/assets/fancyboxJs-BHHTRmwK.js?1765795199" nonce="-JoiRsgFeENKiL1nfI_B3lZoEHx3qBjm-J3EQfgEHVJwHOxaXQzgwQ"></script>
<script type="module" src="/_frontend/bundled/assets/fancyboxCss-CdT9Fa_t.js?1765795199" nonce="-JoiRsgFeENKiL1nfI_B3lZoEHx3qBjm-J3EQfgEHVJwHOxaXQzgwQ"></script>
<script async="async" src="/typo3temp/assets/js/59cb7c5e3fccb968fb36c2e97ef8bf95.js?1765795484"></script>
<script type="module" src="/_frontend/bundled/assets/main-CAt6F8fL.js?1765795199" nonce="-JoiRsgFeENKiL1nfI_B3lZoEHx3qBjm-J3EQfgEHVJwHOxaXQzgwQ"></script>


</body>
</html>